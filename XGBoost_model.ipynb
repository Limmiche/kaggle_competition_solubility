{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from numpy import linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress Convergence and user Warnings\n",
    "\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: randomly choose 500 compounds from the data set\n",
    "# False: everything\n",
    "\n",
    "test_run = False\n",
    "#test_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: predict test data set\n",
    "\n",
    "pred = True\n",
    "#pred = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler during modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: use Standard Scaler\n",
    "\n",
    "StdSca = True\n",
    "#StdSca = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save prediction as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: save prediction as csv file\n",
    "\n",
    "save = False\n",
    "#save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_train_set.csv')\n",
    "df['sol_category'] = pd.to_numeric(df['sol_category'], downcast='integer')\n",
    "\n",
    "if pred:\n",
    "    df_test_set = pd.read_csv('df_test_set.csv')\n",
    "    sub_template = pd.read_csv('Data/submission_template_rdm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking the composition of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sol_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_run:\n",
    "    df, _, __, ___ = train_test_split(df, df['sol_category'], train_size=500, stratify=df['sol_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing model training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining features X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy(deep=True)\n",
    "X.drop(columns=['Id', 'smiles', 'sol_category'], inplace=True)\n",
    "#X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining category list y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining parameters for the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters for the xgboost model\n",
    "params = {}\n",
    "#params['booster'] = 'gbtree' #['gbtree', 'gblinear', 'dart']\n",
    "#params['objective'] = ['binary:logistic']\n",
    "#params[\"eval_metric\"] = [\"error\"]\n",
    "params['eta'] = 0.001 #, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5] # boosting learning rate\n",
    "params['gamma'] = 0.5 #linspace(0.000000001, 1.0, num=11) # min loss red recuired for further partition on leaf node\n",
    "params['max_depth'] = 7 #np.arange(1, 11, 2) # max tree dept for base learners\n",
    "params['n_estimators'] = 100 #np.arange(50, 550, 50) maybe 250 ??\n",
    "params['min_child_weight'] = 1 # min sum of instance weight in a child\n",
    "params['max_delta_step'] = 0 # max delta step allowed for each tree's weight estimate\n",
    "params['subsample']= 0.5 #[0.5, 1] # subsample ratio of training instance\n",
    "params['colsample_bytree'] = 1 # subsample ratio of columns when cunstructing each tree\n",
    "#params['silent'] = [1]\n",
    "#params['seed'] = [0] # = random_state ???\n",
    "params['base_score'] = 0.5 # initial prediction score, global bias\n",
    "#params['random_state'] = [0] # = seed ???\n",
    "#params['scale_pos_weight'] = ratio\n",
    "params['n_jobs'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applying model on training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying StandardScaler\n",
    "if StdSca:\n",
    "    scaler = StandardScaler(copy=True, with_mean=True, with_std=True).fit(X)\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "# splitting data set\n",
    "StratifiedKFold(n_splits=5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1,  shuffle=True, stratify=y)#, test_size=0.2, train_size=0.8)\n",
    "\n",
    "# defining class weights\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train\n",
    "    )\n",
    "\n",
    "# defining model\n",
    "model = XGBClassifier(**params)\n",
    "\n",
    "# fitting model on training data set\n",
    "model.fit(X_train, y_train, sample_weight=classes_weights)\n",
    "\n",
    "# model validation\n",
    "valPredictions = model.predict(X_val)\n",
    "\n",
    "# calculating quadratically weighted kappa score\n",
    "sk_quad_kappa = cohen_kappa_score(y_val, valPredictions, weights='quadratic')\n",
    "    \n",
    " \n",
    "# printing results\n",
    "print(f'quadratically weighted kappa validation score: {sk_quad_kappa}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred:\n",
    "    df_test_set.drop(columns=['Id', 'smiles'], inplace=True)\n",
    "\n",
    "    # apply StandardScaler\n",
    "    if StdSca:\n",
    "        scaler = StandardScaler(copy=True, with_mean=True, with_std=True).fit(df_test_set)\n",
    "        df_test_set = scaler.fit_transform(df_test_set)\n",
    "\n",
    "    # prediction\n",
    "    testPredictions = model.predict(df_test_set)\n",
    "    sub_template['pred'] = testPredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred:\n",
    "    set(testPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred:\n",
    "    sub_template['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    print('saving as csv planned')\n",
    "    #sub_template.to_csv('Submissions/submission_20_12_C-lab.csv', index=False)\n",
    "else:\n",
    "    if test_run:\n",
    "        print('test run')\n",
    "    else:\n",
    "        print('unsaved run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Kaggle_solubility')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "048cebc6bb026d2162f15fb19087d94e922b7f5286c2ebb45fba7f0e79006065"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
